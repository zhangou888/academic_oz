# quadratic term and testing the quadratic to be zero. For linear models,
# this is Tukey's test for nonadditivity when plotting against fitted values.
car::residualPlot(reg4)
# Using 'income' as is.
# Variable 'income' shows some patterns.
# other options:
# Residuals vs fitted only
# get a plot against fitted values only, use the arguments terms = ~ 1
car::residualPlots(reg4, ~ 1, fitted=TRUE)
# Residuals vs education only
car::residualPlots(reg4, ~ education, fitted=FALSE)
# What to look for: No patterns, no problems.
# All p's should be non-significant.
# Model OK if residuals have mean=0 and variance = 1 (Fox, 316)
# Tukey test null hypothesis: model is additive.
# ----  Influential variables- added-variables plots.
reg5 <- lm(prestige ~ education + income + type,
data = Prestige)
# id.n - id most influential observation
# id.cex - font size for id.
car::avPlots(reg5, id.n=2, id.cex=0.7)
# Graphs outcome vs predictor variables holding the rest constant
# (also called partial regression plots)
# Help identify the effect (or influence) of an observation on the regression
# coefficient of the predictor variable
# ---- Outlier - QQ-Plots
reg6 <- lm(prestige ~ education + income + type,
data = Prestige)
# # id.n - id most influential observation
# id.n - id observations with high residuals
car::qqPlot(reg6, id.n=3,envelope=.95)
#-------------------------------------##
# ---- Outliers - Bonferonni Test --- ##
#-------------------------------------##
reg7 <- lm(prestige ~ education + income + type,
data = Prestige)
# null for the Bonferonni adjusted outlier test-the observation is an outlier.
# Here observation related to 'medical.technicians' is an outlier.
car::outlierTest(reg7)
# --- High leverage (hat) point
reg8 <- lm(prestige ~ education + income + type,
data = Prestige)
#-------------------------------------##
#------     Cook's Distance      -----##
#-------------------------------------##
# Cook's distance measures how much an observation influences the overall
# model or predicted values
# Studentized residuals are the residuals divided by their estimated
# standard deviation as a way to standardized
# Bonferroni test to identify outliers
#-------------------------------------##
#-----      Hat-points         -------##
#-------------------------------------##
# Hat-points identify influential observations (have a high impact on the
# predictor variables)
# This function plots 4 different graphs:
# 1. cook's d  2. studentized residual 3. bonferroni test P-value 4. Hat-value
car::influenceIndexPlot(reg8, id.n=3)
# hat-values, and Cook's Distance (represented in circles)
# This function creates a bubble plot of Studentized residuals
# versus hat values, with the areas of the circles representing the
# observations proportional to the value Cook's distance.
# Vertical reference lines are drawn at twice and three times
# the average hat value, horizontal reference lines at -2, 0,
# and 2 on the Studentized-residual scale.
# x-axis: Hat-value (with cutoffs)
# Y-axis: studentized residual
# size of bubble (Cook's D)
car::influencePlot(reg1, id.n=3)
# Breush/Pagan and Cook/Weisberg score test for non-constant error variance.
# Null is constant variance, see also residualPlots(reg1)
car::residualPlots(reg1)
# non-constant variance score test
car::ncvTest(reg1)
# Applying outlierTest function is helping us to
# confirm if potential outliers are indeed outliers.
car::outlierTest(fit1)
# display hatvalues
hatvalues(fit1)
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
# summary(fit)
# Applying outlierTest function is helping us to
# confirm if potential outliers are indeed outliers.
car::outlierTest(fit1)
# This method you choose the variable of interest
car::leveragePlot(weakliem.model1, 'gini')
car::influenceIndexPlot(fit1, id.n=3)
#############################################################################
#                                                                          ##
##          Project: Outliers detection in Regression 2                    ##
##                                                                         ##
##-------------------------------------------------------------------------##
##          Programmer: William G. Jacoby  (MSU)                           ##
##          Request Date: 11-06-2020                                       ##
##          Initial Code:                                                  ##
##          Goals:                                                         ##
##          Input:                                                         ##
##          Output:                                                        ##
##          Note:
##         data : https://quantoid.net/files/702/weakliem2.txt
##-------------------------------------------------------------------------##
##          Modification History:                                          ##
##          When:                                                          ##
##          Who:                                                           ##
##          Change:                                                        ##
##-------------------------------------------------------------------------##
## Step 1: Set work directory
rm(list=ls())
## Step 2: load required packages
packages <- c("tidyverse","ggplot2","outliers","EnvStats",
"mvoutlier","xtable")
packages <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x)
library(x, character.only = TRUE)
}
})
library(ggplot2)
library(outliers)
## Step 3: Set up key libraries and source code
proj.path = file.path("c:/temp/stat ");
data.path = file.path(proj.path,"data/");
out.path = file.path(proj.path,"out/");
setwd(proj.path)
# Ex 1 (use Weakliem.txt DATA)
# Data from World Values Survey 1990.
# Variable- secpay: attitude to two secretaries with the same jobs
# getting paid different amounts if one is better at the job than the other.
# 1=Fair, 2=Unfair.
# gini: the gini coefficient of income inequality in the country.
# 0=perfect equality, 1=perfect inequality.
# gdp: GDP per capita in US dollars;
# democracy: 1=experienced democratic rule for at least 10 years.
# Here we look only at non-democratic countries.
weakliem2 <- read.table("C:/R/projects/outlier/Outlier/data/weakliem.txt",
header=TRUE)
attach(weakliem2)
# check the data structure.
str(weakliem2)
head(weakliem2)
weakliem2 <- weakliem2 %>%
mutate(id = row_number())
plot(gini, secpay, main = "Nondemocratic countries",
xlab="Gini", ylab="Attitudes towards inequality(mean)")
# Model 1
weakliem.model1 <- lm(secpay~gini+gdp, data=weakliem2)
# draw 95% CI ellipse
# from library(car)
# confidenceEllipse(weakliem.model1, levels=0.95,Scheffe=TRUE)
car::dataEllipse(gini, secpay, levels=0.95, lty=1,
main = "Nondemocratic countries",
xlab="Gini", ylab="Attitudes towards inequality(mean)")
# adding regression line
abline(weakliem.model1, lwd=2, lty=1, col=1)
# "identify" remove 'slovakia'(49) and 'CzechRepublic'(25) as outliers.
# Model 2
weakliem.model2 <- update(weakliem.model1, subset=-c(25,49))
abline(weakliem.model2, lwd=2, lty=2, col=2)
# On-time locator (no need to use)
legend(locator(1), lty=1:2, col=1:2,
legend=c('All cases', 'Outliers excluded'))
library(xtable)
print(xtable(weakliem.model1))
print(xtable(weakliem.model2))
# ---- Ex 2: Davis DATA from car --- #
# These data are the Davis data in the car package
library(car)
data(Davis)
attach(Davis)
davis.model.1 <- lm(repwt~weight, data=Davis)
model1 <- lm(weight ~ height, data=Davis)
plot(height, weight, main = "Davis data")
# on-time identify outlier (click on the graph)
identify(height, weight, row.names(Davis))
abline(model1, lty=1, col=1, lwd=3)
# remove outlier from the data then rerun LR model
model2 <- update(model1, subset=-12)
abline(model2, lty=2, col=2, lwd=3)
# On-time locator (no need to use)
legend(locator(1), lty=1:2, col=1:2, lwd=3,
legend=c('All cases', 'Outliers excluded'))
# --------- Types of Unusual Observations ---- #
# ----- 1. Regression Outliers -----
# A regression outlier is an observation that has an
# unusual value of the dependent variable Y,
# conditional on its value of the independent
# variable X.
# ----- 2. Cases with Leverage -----
# An observation that has an unusual X value-i.e., it is far from
# the mean of X-has leverage on (i.e., the potential to influence)
# the regression line
# The further away from from the mean of X (either in a positive or
# negative direction), the more leverage an observation has on the
# regression fit.
# ----- 3. Influential Observations -----
# Only when an observation has high leverage and is an
# outlier in terms of Y-value will it strongly influence
# the regression line
# - In other words, it must have an unusual X-value
# with an unusual Y-value given its X-value
# In such cases both the intercept and slope are affected,
# as the line chases the observation
#  Influence=Leverage X Discrepancy
## ------------------------------------------ ##
## ----------- High leverage -----------------##
# Assessing Leverage: Hat Values (1)
# Most common measure of leverage is the hat-value
# ---- R script for plot of Hat Values --- #
plot(hatvalues(weakliem.model1), main="Hat Values for Ineqaulity model")
# Draw Hat value cutoff.
abline(h=c(2,3)*3/length(secpay), lty=2)
# "h" signifies horizontal line
# the average hat value = (k+1)/n
# a Rule of thumb is that 2*average hat value
# for small samples should be examined
# These cases have high leverage, but not necessarily high influence.
graphics::identify(1:length(secpay),
hatvalues(weakliem.model1),
row.names(weakliem2))
## --------------------------------------------- ##
# Formal Tests for Outliers: Standardized Residuals
## --------------------------------------------- ##
# high leverage observations can
# have small residuals because they pull the line towards them.
out1 <- outlierTest(weakliem.model1)
(out1)
weakliem2$country[49]
weakliem.dfbetas <- dfbetas(weakliem.model1)
# weakliem.dfbetas
# unclass(weakliem.dfbetas)
# c(2,3) specifies the coefficients of interests
plot(weakliem.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Gini and GDP coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(weakliem2)), lty=2)
knitr::include_graphics("figure/high_hat_value.png", error = FALSE)
# data states
# fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
# You can compute the high leverage observation by looking at the
# ratio of number of parameters estimated in model and sample size.
# If an observation has a ratio greater than 2-3 times the average ratio,
# then the observation considers as high-leverage points.
# high leverage function
p <- length(coefficients(fit1)) # number of IVs + Intercept
# 2. Added Variable plots
# It can be easily created using the avPlots() function in car package.
# From the graph below, the straight line in each plot is the actual
# regression coefficient for that predictor variable.
car::avPlots(fit1, ask=FALSE, id.method="identify")
library(car)
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
# summary(fit)
# From our regression model example, we can start
# investigating outliers observation by using Q-Q plot.
car::qqPlot(fit1,labels=row.names(states), id.method="identify",
simulate=TRUE, main="Q-Q Plot")
# 2. Added Variable plots
# It can be easily created using the avPlots() function in car package.
# From the graph below, the straight line in each plot is the actual
# regression coefficient for that predictor variable.
car::avPlots(fit1, ask=FALSE, id.method="identify")
# data states
# fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
# You can compute the high leverage observation by looking at the
# ratio of number of parameters estimated in model and sample size.
# If an observation has a ratio greater than 2-3 times the average ratio,
# then the observation considers as high-leverage points.
# high leverage function
p <- length(coefficients(fit1)) # number of IVs + Intercept
n <- length(fitted(fit1))       # number of observations (N)
# 2-3 times average ratio as cutoff
ratio <- p/n
# display hatvalues
hatvalues(fit1)
# High leverage function.
highleverage <- function(fit) {
p <- length(coefficients(fit)) # number of IVs + Intercept
n <- length(fitted(fit))       # number of observations (N)
ratio <- p/n
# leverage ratio
plot(hatvalues(fit), main="Index Plot of Ratio")
# 2-3 times the average ratio as cutoff lines.
abline(h=c(2,3)*ratio, col="red", lty=2)
# identify high leverage points from graph.
graphics::identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}
# run function
# Since the graphics::identify function is included, we will not actually run this code in the rmarkdown.
# highleverage(fit1)
fit1
fit1.dfbetas <- dfbetas(fit1)
# c(2,3) specifies the coefficients of interests
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population, Illiteracy, Income, and Frost coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
data("States")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(States)), lty=2)
states
length(states)
weakliem2 <- read.table("C:/R/projects/outlier/Outlier/data/weakliem.txt",
header=TRUE)
attach(weakliem2)
length(weakliem2)
length(states)
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
## an influential observation is one that
#  combines discrepancy with leverage.
fit1.dfbetas <- dfbetas(fit1)
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
2/sqrt(length(states))
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
cut <- h=2/sqrt(length(states))
cut <- 2/sqrt(length(states))
# adds the rule of thumb cut-off line
abline(h=cut, lty=2)
# adds the rule of thumb cut-off line
abline(cut, lty=2)
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
## an influential observation is one that
#  combines discrepancy with leverage.
fit1.dfbetas <- dfbetas(fit1)
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
# data states
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
## an influential observation is one that
#  combines discrepancy with leverage.
fit1.dfbetas <- dfbetas(fit1)
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
plot.new()
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
# data states
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
## an influential observation is one that
#  combines discrepancy with leverage.
fit1.dfbetas <- dfbetas(fit1)
# c(2,3) specifies the coefficients of interests (Population and illiteracy)
plot(fit1.dfbetas[,c(2,3)],
xlim = c(-2,2),ylim=c(-1,1),
main="DFBetas for the Population and Illiteracy coefficients")
# adds the rule of thumb cut-off line
abline(h=2/sqrt(length(states)), lty=2)
# identify influential points
graphics::identify(weakliem.dfbetas[,2], weakliem.dfbetas[,3],
weakliem2$country)
cutoff <- 4/(nrow(states)-length(fit$coefficients)-2)
plot(fit, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
highleverage <- function(fit) {
p <- length(coefficients(fit)) # number of IVs + Intercept
n <- length(fitted(fit))       # number of observations (N)
ratio <- p/n
# leverage ratio
plot(hatvalues(fit), main="Index Plot of Ratio")
# 2-3 times the average ratio as cutoff lines.
abline(h=c(2,3)*ratio, col="red", lty=2)
# identify high leverage points from graph.
graphics::identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}
# run function
highleverage(fit)
#############################################################################
#                                                                          ##
##          Project: Outliers detection in Regression 3                    ##
##                                                                         ##
##-------------------------------------------------------------------------##
##          Programmer:  Michaelino Mervisiano
##
##          Request Date: 11-10-2020                                       ##
##          Initial Code:                                                  ##
##          Goals:                                                         ##
##          Input:                                                         ##
##          Output:                                                        ##
##          Note: This chapter explains the purpose of some of the most    ##
##         commonly used outlier analysis and how to implement them in R   ##
##  https://towardsdatascience.com/how-to-detect-unusual-observations-on-your-regression-model-with-r-de0eaa38bc5b ##
##         The R content presented in this document is mostly based on an  ##
##-------------------------------------------------------------------------##
##          Modification History:                                          ##
##          When: 11-10-2020                                               ##
##          Who:   Ou Zhang                                                ##
##          Change:                                                        ##
##-------------------------------------------------------------------------##
## Step 1: Set work directory
rm(list=ls())
## Step 2: load required packages
packages <- c("tidyverse","ggplot2","outliers",
"mvtnorm","ellipse","car",
"EnvStats","mvoutlier")
packages <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x)
library(x, character.only = TRUE)
}
})
## Step 3: Set up key libraries and source code
proj.path = file.path("c:/temp/stat ");
data.path = file.path(proj.path,"data/");
out.path = file.path(proj.path,"out/");
setwd(proj.path)
# How to Identify Unusual Observations in Regression?
# Created by Michaelino Mervisiano
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
summary(fit)
# Outliers
# Outliers observations aren't predicted well by regression model.
# From our regression model example, we can start
# investigating outliers observation by using Q-Q plot.
car::qqPlot(fit,labels=row.names(states), id.method="identify",
simulate=TRUE, main="Q-Q Plot")
# Applying outlierTest function is helping us to
# confirm if potential outliers are indeed outliers.
car::outlierTest(fit)
# -- Check 'Nevada' real murder rate and fitted murder rate
states["Nevada",]
fitted(fit)["Nevada"]
cutoff <- 4/(nrow(states)-length(fit$coefficients)-2)
plot(fit, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
##  Influence = Leverage x Discrepancy
# 1. Cook's distance
cutoff <- 4/(nrow(states)-length(fit1$coefficients)-2)
plot(fit1, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
##  Influence = Leverage x Discrepancy
# 1. Cook's distance
cutoff <- 4/(nrow(states)-length(fit1$coefficients)-2)
plot(fit1, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
states <- as.data.frame(state.x77[,c("Murder", "Population",
"Illiteracy", "Income", "Frost")])
# fit the data into the linear regression model
fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
fit1_cookd <- cooks.distance(fit1)
fit1_cookd
blogdown:::serve_site()
blogdown:::serve_site()
xaringan:::inf_mr()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
xaringan:::inf_mr()
blogdown:::serve_site()
xaringan:::inf_mr()
blogdown:::serve_site()
blogdown:::serve_site()
