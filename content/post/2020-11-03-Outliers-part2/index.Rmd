---
title: Outliers-Part 2:Finding Outliers in a univariated way
author: Ou Zhang
date: '2020-11-03'
categories:
  - R
  - Statistics
tags:
  - R
  - Statistics
slug: outliers-part2
draft: no
lastmod: '2020-11-03T23:53:01-05:00'
featured: no
disable_jquery: no
image:
  caption: ''
  focal_point: Smart
  preview_only: no
output:
  blogdown::html_page:
    toc: yes
    number_sections: yes
    toc_depth: 1
    self_contained: no

---

```{r set-global-options, echo = FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = TRUE,
                      # root.dir = "C:/Users/uzhanou/Documents/R/markdown/Serena/",
                      fig.path = "figure/",  # Set the figure options
                      fig.align = "center", 
                      fig.width = 6,
                      fig.height = 6)
```

```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}
## Step 2: load required packages 
packages <- c("tidyverse","mlbench","ggplot2","mice","Hmisc","DMwR",
              "rpart","mice","outliers", "knitr","EnvStats","mvoutlier")
packages <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x)
    library(x, character.only = TRUE)
  }  
})

```

```{r fig1, out.width='70%', fig.align='center', fig.pos='h', fig.cap='Outliers',echo = FALSE}

    knitr::include_graphics("figure/tenor.gif", error = FALSE)

```

In this 2nd part, we are going to discuss more technical details of the outlier detection. **Outliers** are one of those statistical issues that everyone knows about, but most people aren’t sure how to deal with. Most parametric statistics, like **means, standard deviations, and correlations**, and **every statistic based on these**, are _highly sensitive_ to outliers.

Finding outliers depends on subject-area knowledge and an understanding of the data collection process. While there is no solid mathematical definition, there are guidelines and statistical tests you can use to find outlier candidates.

Outlier is a simple concept: 

  > **they are values that are notably different from other data points, and they can cause problems in statistical procedures.**
  
  
Removing or keeping an outlier depends on

  (i) the context of your analysis,
  (ii) whether the tests you are going to perform on the dataset are robust to outliers or not, and
  (iii) how far is the outlier from other observations.

In general, there are two different types of outliers- __univariate__ and __multivariate__ outliers.

 - A `univariate outlier` is a data point that consists of an extreme value on `one variable`.
 - A `multivariate outlier` is a combination of unusual scores on `at least two variables`. 

Because of the outlider different types, outlier detection methods can be divided between `univariate methods` and `multivariate methods` that usually for most of the current body of research. 


In this post, I am going to focus on the `univariate methods` only.

Generally, there are 7 different types of methods to detect outliers **univariately**.


# Method 1: Sorting Your Datasheet to Find Outliers

Sorting your datasheet is a simple but `effective` way to highlight unusual values. Simply sort your data sheet for each variable and then look for unusually high or low values.

See the example below. 


```{r example1}

# Example data.
ex1 <- data.frame(id=c(1,2,3,4,5),
                  gender=c("M","F","F","M","M"),
                  age=c(15, 20, 400, 27, 50))

# sort data by age.
arrange(ex1,age)

```

As we all know, human's age cannot be 400. So, we can easily detect that case (id=3) is an outlier. It is caused either from data-entry error or something else.


# Method 2: Graphing Your Data to Identify Outliers

Boxplots, histograms, and scatterplots can highlight outliers.

   - univariate outliers: `Boxplot` (interquartile method with fence), `histograms`
   - Bivariate/multivariate outliers: `scatterplots` 

The scatterplot with regression line shows how most of the points follow the fitted line for the model. However, the circled point does not fit the model well(`We will discuss this in the next post`).

This type of outlier can be a problem in regression analysis. Given the multifaceted nature of multivariate regression, there are numerous types of outliers in that realm. 

## Histogram

One basic way to detect outliers is to draw a histogram of the data.

```{r histogram}

# use example data
dat <- ggplot2::mpg
summary(dat$hwy)

ggplot(dat) +
  aes(x = hwy) +
  geom_histogram(bins = 30L, fill = "#0c4c8a") +
  theme_minimal()

```

From the histogram, there seems to be a couple of observations higher than all other observations (see the bar on the right side of the plot).

## Boxplot
In addition to histograms, boxplots are also useful to detect potential outliers. A boxplot helps to visualize a quantitative variable by displaying five common location summary (minimum, median, first and third quartiles and maximum) and any observation that was classified as a suspected outlier using the interquartile range (IQR) criterion (See the method 4). 

```{r boxplot}

# using ggplot2:
ggplot(dat) + aes(x = "", y = hwy) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()

```

# Method 3: Using Z-scores to Detect Outliers

Z-scores can quantify the unusualness of an observation when your data follow the normal distribution. Z-scores are the number of standard deviations above and below the mean that each value falls.

To calculate the Z-score for an observation, take the raw measurement, subtract the mean, and divide by the standard deviation. Mathematically, the formula for that process is the following:

$$Z=\frac{X-\mu}{\sigma}$$

The further away an observation’s Z-score is from zero, the more unusual it is. A standard cut-off value for finding outliers are Z-scores of `+/-3` or further from zero.  

However, **if your data don’t follow the normal distribution, this approach might not be accurate.**

Note that Z-scores can be misleading with small datasets because the maximum Z-score is limited to $\frac{n-1}{\sqrt{n}}$.

Also, note that the outlier’s presence throws off the Z-scores because it inflates the mean and standard deviation as we saw earlier. Notice how all the Z-scores are negative except the outlier’s value. If we calculated Z-scores without the outlier, they’d be different! Be aware that if your dataset contains outliers, Z-values are biased such that they appear to be less extreme (i.e., closer to zero).


# Method 4: Using the Interquartile Range (IRQ) to Create Outlier Fences

You can use the `interquartile range (IQR)`, several quartile values, and an adjustment factor to calculate boundaries for what constitutes minor and major outliers. Minor and major denote the unusualness of the outlier relative to the overall distribution of values. Major outliers are more extreme. Analysts also refer to these categorizations as mild and extreme outliers.

The IQR is the middle `50%` of the dataset. It’s the range of values between the **third quartile** and the **first quartile (Q3 – Q1)**. 

We can take the IQR, Q1, and Q3 values to calculate the following outlier fences for our dataset: lower outer, lower inner, upper inner, and upper outer. These fences determine whether data points are outliers and whether they are mild or extreme.
Values that fall inside the two inner fences are not outliers.

To calculate the outlier fences, do the following:

   1.	Take your IQR and multiply it by `1.5` and `3`. We’ll use these values to obtain the inner and outer fences. For our example, the IQR equals 0.222. Consequently, 0.222 * 1.5 = 0.333 and 0.222 * 3 = 0.666. We’ll use 0.333 and 0.666 in the following steps.

   2.	Calculate the **inner** and **outer lower fences**. Take the Q1 value and subtract the two values from step 1. The two results are the lower inner and outer outlier fences. For our example, Q1 is 1.714. So, the lower inner fence = 1.714 – 0.333 = 1.381 and the lower outer fence = 1.714 – 0.666 = 1.048.

   3.	Calculate the inner and outer upper fences. Take the Q3 value and add the two values from step 1. The two results are the upper inner and upper outlier fences. For our example, Q3 is 1.936. So, the upper inner fence = 1.936 + 0.333 = 2.269 and the upper outer fence = 1.936 + 0.666 = 2.602.

The IQR method is helpful because it uses percentiles, which do not depend on a specific distribution. Additionally, percentiles are relatively robust to the presence of outliers compared to the other quantitative methods.

It is also possible to extract the values of the potential outliers based on the IQR criterion thanks to the `boxplot.stats()$out` function:

```{r IRQ1}

boxplot.stats(dat$hwy)$out

# Thanks to the which() function it is possible to extract the row number
# corresponding to these outliers:
out <- boxplot.stats(dat$hwy)$out
out_ind <- which(dat$hwy %in% c(out))
out_ind

```

With this information you can now easily go back to the specific rows in the dataset to verify them, or print all variables for these outliers. It is also possible to print the values of the outliers directly on the boxplot with the `mtext()` function:

```{r IRQ2}

dat[out_ind, ]
boxplot(dat$hwy,
        ylab = "hwy",
        main = "Boxplot of highway miles per gallon")

# print out outlier
mtext(paste("Outliers: ", paste(out, collapse = ", ")))

```

# Method 5: Percentiles

This method of outliers detection is based on the percentiles. With the percentiles method, all observations that lie outside the interval formed by the `2.5` and `97.5` percentiles will be considered as potential outliers. Other percentiles such as the 1 and 99, or the 5 and 95 percentiles can also be considered to construct the interval.

The values of the lower and upper percentiles (and thus the lower and upper limits of the interval) can be computed with the `quantile()` function:

```{r quantile1}
lower_bound <- quantile(dat$hwy, 0.025)
lower_bound

upper_bound <- quantile(dat$hwy, 0.975)
upper_bound
```

According to this method, all observations below 14 and above `35.175` will be considered as potential outliers. The row numbers of the observations outside of the interval can then be extracted with the `which()` function:

```{r quantile2}
# So the outlier will be
outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)
outlier_ind
```

Then their values of highway miles per gallon can be printed:
```{r quantile3}
dat[outlier_ind, "hwy"]
```

Alternatively, all variables for these outliers can be printed:
```{r quantile4}
dat[outlier_ind, ]
```
There are `11` potential outliers according to the percentiles method. To reduce this number, you can set the percentiles to `1` and `99`:

```{r quantile5}
lower_bound <- quantile(dat$hwy, 0.01)
upper_bound <- quantile(dat$hwy, 0.99)

outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)

# print out outlier data
dat[outlier_ind, ]

```

# Method 6: Hampel filter
Another method, known as Hampel filter, consists of considering as outliers the values outside the interval($I$) formed by the `median`, plus or minus `3 median absolute deviations` ($MAD$): (default as 3, could be different)

$$I=[median-3\times MAD, median+3\times MAD]$$
where $MAD$ is the median absolute deviation and is defined as the median of the absolute deviations from the data’s median:

$$MAD = median(|X_{i} - median(X)|)$$
For this method we first set the interval limits thanks to the `median()` and `mad()` functions:

```{r Hampel1}
median(dat$hwy)
mad(dat$hwy)

lower_bound <- median(dat$hwy) - 3 * mad(dat$hwy)
lower_bound

upper_bound <- median(dat$hwy) + 3 * mad(dat$hwy)
upper_bound

```

According to this method, all observations below `1.761` and above `46.239` will be considered as potential outliers. The row numbers of the observations outside of the interval can then be extracted with the `which()` function:

```{r Hampel2}
outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)
outlier_ind
```

According to the Hampel filter, there is no potential outlier for the `hwy` variable.

# Method 7: Finding Outliers with Hypothesis Tests

You can use hypothesis tests to find outliers. Many outlier tests exist, but I’ll focus on one to illustrate how they work. In this post, 3 more formal techniques to detect outliers:

 - Grubbs’s test
 - Dixon’s test
 - Rosner’s test

These 3 statistical tests are part of more formal techniques of outliers detection as they all involve the computation of a test statistic that is compared to tabulated critical values (that are based on the sample size and the desired confidence level).

Note that the 3 tests are appropriate only when the data (without any outliers) are **approximately normally distributed**. The normality assumption must thus be verified before applying these tests for outliers.

## Grubbs' test

The Grubbs test allows to detect whether the highest or lowest value in a dataset is an outlier.

The Grubbs test detects one outlier at a time (highest or lowest value), so the null and alternative hypotheses are as follows.

  - $H_{0}$: The _highest_ value is **not** an outlier
  -	$H_{1}$: The _highest_ value is an outlier.
 
if we want to test the highest value, or:

  - $H_{0}$: The _lowest_ value is **not** an outlier
  -	$H_{1}$: The _lowest_ value is an outlier.
  
if we want to test the lowest value.

If the p-value for this test is less than your significance level, you can reject the null and conclude that one of the values is an outlier. The analysis identifies the value in question.

If you use Grubbs’ test and find an outlier, **don’t remove that outlier and perform the analysis again**. That process can cause you to remove values that are not outliers.

**Note that the Grubbs test is not appropriate for sample size of 6 or less.**

$$n\leq6$$
To perform the Grubbs test in R, we use the `grubbs.test(`) function from the `{outliers}` package:


```{r Grubb1}
library(outliers)
test <- grubbs.test(dat$hwy)
test

```

If you want to do the test for the lowest value, simply add the argument `opposite = TRUE` in the `grubbs.test()` function:


```{r Grubb2}
test <- grubbs.test(dat$hwy, opposite = TRUE)
test
```

For the sake of illustration, we will now replace an observation with a more extreme value and perform the Grubbs test on this new dataset. Let's replace the 35th row with a value of 230:

```{r Grubb3}
dat[35, "hwy"] <- 230

test <- grubbs.test(dat$hwy)
test

```

The p-value is `< 0.001`. At the `5%` significance level, we conclude that the highest value 230 is an outlier.

## Dixon’s test

Similar to the Grubbs test, Dixon test is used to test whether a single low or high value is an outlier. So if more than one outliers is suspected, the test has to be performed on these suspected outliers individually.

Note that- Dixon test is most useful for small sample size (usually $n\leq25$).

To perform the Dixon’s test in R, we use the `dixon.test()` function from the `{outliers}` package. However, we restrict our dataset to the `20` first observations as the Dixon test can only be done on small sample size (R will throw an error and accepts `only dataset of 3 to 30 observations)`:

```{r dixon1}
subdat <- dat[1:25, ]
test <- dixon.test(subdat$hwy)
test
```

The results show that the lowest value 15 is an outlier (_p_-value = 0.007).

To test for the highest value, simply add the `opposite = TRUE` argument to the `dixon.test()` function:

```{r dixon2}
test <- dixon.test(subdat$hwy, opposite = TRUE)
test
```

The results show that the highest value `31` is not an outlier (_p_-value = 0.858).

It is a good practice to always check the results of the statistical test for outliers against the boxplot to make sure we tested all potential outliers:

```{r dixon3}
out <- boxplot.stats(subdat$hwy)$out
boxplot(subdat$hwy,
        ylab = "hwy")
mtext(paste("Outliers: ", paste(out, collapse = ", ")))
```

From the boxplot, we see that we could also apply the Dixon test on the value 20 in addition to the value 15 done previously. This can be done by finding the row number of the minimum value, excluding this row number from the dataset and then finally apply the Dixon test on this new dataset:

```{r dixon4}
# find and exclude lowest value
remove_ind <- which.min(subdat$hwy)
subsubdat <- subdat[-remove_ind, ]

# Dixon test on dataset without the minimum
test <- dixon.test(subsubdat$hwy)
test
```
The results show that the second lowest value 20 is not an outlier (_p_-value = 0.13).

## Rosner’s test

Rosner’s test for outliers has the advantages that:

  1. it is used to detect several outliers at once (unlike `Grubbs` and `Dixon test` which must be performed iteratively to screen for multiple outliers), and
  2. it is designed to avoid the problem of masking, where an outlier that is close in value to another outlier can go undetected.

Unlike Dixon test, note that Rosner test is most appropriate __when the sample size is large __ ($n\geq20$). We therefore use the initial dataset, which includes 234 observations.

To perform the Rosner test we use the `rosnerTest()` function from the `{EnvStats}` package. 
This function requires at least 2 arguments: 

  - the data and 
  - the number of suspected outliers `k` (with k = 3 as the default number of suspected outliers).

For this example, we set the number of suspected outliers to be equal to 3, as suggested by the number of potential outliers outlined in the boxplot.

```{r rosner1}
library(EnvStats)
test <- rosnerTest(dat$hwy, k = 3)
test
```

The interesting results are provided in the `$all.stats` table:

```{r rosner2}
test$all.stats
```

## Challenges of Using Outlier Hypothesis Tests: `Masking` and `Swamping`

When performing an outlier test, you either need to choose a procedure based on the number of outliers or specify the number of outliers for a test. `Grubbs’ test` checks for only one outlier. However, other procedures, such as the `Tietjen-Moore Test`, require you to specify the number of outliers. That’s hard to do correctly! After all, you’re performing the test to find outliers! Masking and swamping are two problems that can occur when you specify the incorrect number of outliers in a dataset.

 - `Masking` occurs when you specify too few outliers. The additional outliers that exist can affect the test so that it detects no outliers. For example, if you specify one outlier when there are two, the test can miss both outliers.

 - Conversely, `swamping` occurs when you specify too many outliers. In this case, the test identifies too many data points as being outliers. For example, if you specify two outliers when there is only one, the test might determine that there are two outliers.
 
 
 
Note: Some of the contents are originally from Jim Frost's and Antoine Soetewey  online tutorials. If you are interested in, you can find both online tutorials below.

[Jim Frost](https://statisticsbyjim.com/basics/outliers/#:~:text=Note%20that%20Z%2Dscores%20can,cutoff%20value%20of%20%2B%2F%2D3)

[Antoine Soetewey](https://www.statsandr.com/blog/outliers-detection-in-r/)


-- To be Continued










