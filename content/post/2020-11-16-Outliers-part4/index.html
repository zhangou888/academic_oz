<link href="index_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections-1.0/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#data-source"><span class="toc-section-number">1</span> Data Source</a><ul>
<li><a href="#variables-in-data"><span class="toc-section-number">1.1</span> Variables in Data</a></li>
</ul></li>
<li><a href="#model-specific-methods"><span class="toc-section-number">2</span> Model-specific methods</a><ul>
<li><a href="#cooks-distance"><span class="toc-section-number">2.1</span> Cook’s Distance</a></li>
<li><a href="#pareto"><span class="toc-section-number">2.2</span> Pareto</a></li>
</ul></li>
<li><a href="#multivariate-methods"><span class="toc-section-number">3</span> Multivariate methods</a><ul>
<li><a href="#mahalanobis-distance"><span class="toc-section-number">3.1</span> Mahalanobis Distance</a><ul>
<li><a href="#details-about-mahalanobis-distance"><span class="toc-section-number">3.1.1</span> Details about Mahalanobis Distance</a></li>
</ul></li>
<li><a href="#robust-mahalanobis-distance"><span class="toc-section-number">3.2</span> Robust Mahalanobis Distance</a></li>
<li><a href="#minimum-covariance-determinant-mcd"><span class="toc-section-number">3.3</span> Minimum Covariance Determinant (MCD)</a><ul>
<li><a href="#robust-tolerance-ellipsoid-rte"><span class="toc-section-number">3.3.1</span> robust tolerance ellipsoid (RTE)</a></li>
</ul></li>
<li><a href="#invariant-coordinate-selection-ics"><span class="toc-section-number">3.4</span> Invariant Coordinate Selection (ICS)</a></li>
<li><a href="#optics"><span class="toc-section-number">3.5</span> OPTICS</a></li>
<li><a href="#isolation-forest"><span class="toc-section-number">3.6</span> Isolation Forest</a></li>
<li><a href="#local-outlier-factor"><span class="toc-section-number">3.7</span> Local Outlier Factor</a></li>
</ul></li>
<li><a href="#check_outliers-function-in-performance-r-package"><span class="toc-section-number">4</span> ‘check_outliers’ function in {performance} R package</a><ul>
<li><a href="#threshold-specification"><span class="toc-section-number">4.0.1</span> Threshold specification</a></li>
</ul></li>
<li><a href="#reference"><span class="toc-section-number">5</span> Reference</a></li>
</ul>
</div>

<div class="figure" style="text-align: center"><span id="fig:fig1"></span>
<img src="figure/outlier.jpg" alt="Outliers" width="60%" />
<p class="caption">
Figure 0.1: Outliers
</p>
</div>
<p>In my last blog, I’ve discussed the outliers detection in regression (<a href="/2020/11/09/outliers-part3/">part 3</a>). In fact, outliers in regression is a special case of the multivariate outliers. Because regression has been widely used, I choose to make it as a special topic.</p>
<p>Now, in this part I am going to discuss how the <strong>multivariate outliers</strong> are detected in a general way.</p>
<p>In this post, a linear regression example is still used, but the outlier detection methods we apply are in a multivariate way.</p>
<blockquote>
<p>Multivariate Statistics - Simultaneous observation and analysis of more than one outcome variable.</p>
</blockquote>
<div id="data-source" class="section level1">
<h1><span class="header-section-number">1</span> Data Source</h1>
<p>The <code>Animals</code> data from the <code>MASS</code> package in R is selected for demonstration.</p>
<pre class="r"><code># Load library and pull out data
library(MASS)
data(Animals)

attach(Animals)

# check data structure.
head(Animals)</code></pre>
<pre><code>##                     body brain
## Mountain beaver     1.35   8.1
## Cow               465.00 423.0
## Grey wolf          36.33 119.5
## Goat               27.66 115.0
## Guinea pig          1.04   5.5
## Dipliodocus     11700.00  50.0</code></pre>
<div id="variables-in-data" class="section level2">
<h2><span class="header-section-number">1.1</span> Variables in Data</h2>
<p>The variables for the demonstration are <strong>body weight</strong> and <strong>brain weight</strong> of Animals which are converted to its <code>log form</code> (to make highly skewed distributions less skewed)</p>
<pre class="r"><code># Log transformation.
Y &lt;- data.frame(body = log(Animals$body), 
                brain = log(Animals$brain))

# Create Scatterplot
plot_fig &lt;- ggplot(Y, aes(x = body, y = brain)) + 
            geom_point(size = 5) +
                       xlab(&quot;log(body)&quot;) + 
                       ylab(&quot;log(brain)&quot;) + 
                       ylim(-5, 15) +
            scale_x_continuous(limits = c(-10, 16), breaks = seq(-15, 15, 5))

plot_fig</code></pre>
<p><img src="figure/data_convert-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>In the multivariate world, outliers can be defined as particularly <strong>influential observations</strong>.</p>
<blockquote>
<p>An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” – D. M. Hawkins</p>
</blockquote>
<p>Most multivariate outlier detection methods rely on the computation of some <em>distance metric</em>, and the observations greater than a <strong>certain threshold</strong> are considered outliers.</p>
<p>Importantly, outliers detection methods are meant to provide information to the researcher, rather than being an automatized procedure which mindless application is a substitute for thinking.</p>
<p>Again, outlier detection and handling is still a <strong>subjective process</strong> from researcher. It varies based on different research goals, perspectives, and theoretical frameworks.</p>
<p>Using the composite approach, multivariate outliers are obtained via the joint application of multiple outliers detection algorithms listed below:</p>
<ul>
<li>Mahalanobis distance</li>
<li>Robust Mahalanobis distance<br />
</li>
<li>Minimum Covariance Determinant<br />
</li>
<li>Invariant Coordinate Selection</li>
<li>OPTICS</li>
<li>Isolation Forest</li>
<li>Local Outlier Factor</li>
</ul>
</div>
</div>
<div id="model-specific-methods" class="section level1">
<h1><span class="header-section-number">2</span> Model-specific methods</h1>
<p>Prior to the general multivariate outlier detection method introduction, it is worth mentioning a couple of model-specific methods.</p>
<div id="cooks-distance" class="section level2">
<h2><span class="header-section-number">2.1</span> Cook’s Distance</h2>
<p>Among outlier detection methods, Cook’s distance and leverage are less common than the basic <strong>Mahalanobis distance</strong>, but still used. Cook’s distance estimates the variations in regression coefficients after removing each observation, one by one (Cook, 1977). Since Cook’s distance is in the metric of an F distribution with p and n-p degrees of freedom, the median point of the quantile distribution can be used as a cut-off (Bollen, 1985). A common approximation or heuristic is to use <code>4 divided by the numbers of observations</code>, which usually corresponds to a lower threshold (i.e., more outliers are detected). This only works for Frequentist models. For more Cook’s Distance information, please refer my previous blog (<a href="/2020/11/09/outliers-part3/">part 3</a>).</p>
</div>
<div id="pareto" class="section level2">
<h2><span class="header-section-number">2.2</span> Pareto</h2>
<p>Pareto is a coefficient used for for Bayesian models. The reliability and approximate convergence of Bayesian models can be assessed using the estimates for the shape parameter <span class="math inline">\(k\)</span> of the generalized Pareto distribution. If the estimated tail shape parameter k exceeds <code>0.5</code>, the user should be warned, although in practice the authors of the loo package observed good performance for values of <span class="math inline">\(k\)</span> up to <code>0.7</code> (the default threshold used by performance).</p>
</div>
</div>
<div id="multivariate-methods" class="section level1">
<h1><span class="header-section-number">3</span> Multivariate methods</h1>
<div id="mahalanobis-distance" class="section level2">
<h2><span class="header-section-number">3.1</span> Mahalanobis Distance</h2>
<p><strong>Mahalanobis distance</strong> (Mahalanobis, 1930) is often used for multivariate outliers detection as this distance takes into account the shape of the observations.</p>
<p>Mahalanobis (or generalized) distance for observation is the <strong>distance from this observation to the center</strong>, <em>taking into account the covariance matrix</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig2"></span>
<img src="figure/mahalanobis.png" alt="Outliers" width="70%" />
<p class="caption">
Figure 3.1: Outliers
</p>
</div>
<div id="details-about-mahalanobis-distance" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Details about Mahalanobis Distance</h3>
<ul>
<li><p>Classical Mahalanobis distances: sample mean as estimate for location and sample covariance matrix as estimate for scatter.</p></li>
<li><p>To detect multivariate outliers the Mahalanobis distance is compared with a cut-off value, which is derived from the <code>chi-square distribution</code></p></li>
<li><p>In two dimensions we can construct corresponding <code>97.5%</code> tolerance ellipsoid, which is defined by those observations whose Mahalanobis distance does not exceed the cut-off value.</p></li>
</ul>
<p>The <code>default threshold</code> is often arbitrarily set to some deviation (in terms of SD or MAD) from the mean (or median) of the Mahalanobis distance. However, as the Mahalanobis distance can be approximated by a <code>Chi-squared distribution</code> (Rousseeuw &amp; Van Zomeren, 1990), we can use the <code>alpha quantile</code> of the chi-square distribution with <code>k</code> degrees of freedom (<span class="math inline">\(k\)</span> being the number of columns).</p>
<p>By default, the <code>alpha threshold</code> is set to <code>0.025</code> (corresponding to the <strong>2.5%</strong> most extreme observations; Cabana, 2019). This criterion is a natural extension of the median plus or minus a coefficient times the MAD method (Leys et al., 2013).</p>
<pre class="r"><code>Y_center &lt;- colMeans(Y)
Y_cov &lt;- cov(Y)
Y_radius &lt;- sqrt(qchisq(0.975, df = ncol(Y)))
library(car)
Y_ellipse &lt;- data.frame(ellipse(center = Y_center,
                                shape = Y_cov,radius = Y_radius, 
                                segments = 100, draw = FALSE))

colnames(Y_ellipse) &lt;- colnames(Y)

plot_fig &lt;- plot_fig +
  geom_polygon(data=Y_ellipse, color = &quot;dodgerblue&quot;,
               fill = &quot;dodgerblue&quot;, alpha = 0.2) +
  geom_point(aes(x = Y_center[1], y = Y_center[2]),
             color = &quot;blue&quot;, size = 6)
plot_fig</code></pre>
<p><img src="figure/mahalanobis-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As you can observed from the plot graph above, Mahalanobis Distance method gives us <code>3</code> potential outlier observations, which are close to the ellipse line.</p>
<p>The <code>maha_dist</code> function from <code>{assertr}</code> computes mahalanobis distance for each row of data frame. This function will return a vector, with the same length as the number of rows of the provided data frame, corresponding to the average mahalanobis distances of each row from the whole data set.</p>
<pre class="r"><code>library(assertr)
maha_dist(mtcars)</code></pre>
<pre><code>##  [1]  8.946673  8.287933  8.937150  6.096726  5.429061  8.877558
##  [7]  9.136276 10.030345 22.593116 12.393107 11.058878  9.476126
## [13]  5.594527  6.026462 11.201310  8.672093 12.257618  9.078630
## [19] 14.954377 10.296463 13.432391  6.227235  5.786691 11.681526
## [25]  6.718085  3.645789 18.356164 14.000669 21.573003 11.152850
## [31] 19.192384  9.888781</code></pre>
<pre class="r"><code>maha_dist(iris, robust=TRUE)</code></pre>
<pre><code>##   [1]  7.181303 14.209332  9.233412 14.029571  6.547538  9.032628
##   [7]  9.557745  9.434082 16.327924 14.609934  7.944119 12.241370
##  [13] 14.152604 10.634152  8.940285  9.311006  7.996840  7.676170
##  [19] 10.686693  6.772707 14.171183  8.199367  4.819010 17.927827
##  [25] 21.970559 17.509804 12.134829  8.718618  8.664029 14.539774
##  [31] 15.467206 13.918214 12.751421  7.800228 13.513486  9.613279
##  [37]  9.535921  8.718478 13.079168  9.504772  7.041884 36.721567
##  [43] 10.581647 18.598838 14.208248 15.428511  9.871731 10.898606
##  [49]  7.630712  9.183846 12.142542  5.700266 15.153194  9.430086
##  [55] 12.108781 11.172977  9.444704  5.395403 11.484362  7.646899
##  [61] 10.479824  5.254603 14.996254 11.744912  2.849079  6.677997
##  [67] 10.917922  9.978773 24.822657  4.694515 20.355909  3.146007
##  [73] 22.559940 18.395726  5.389387  6.864585 17.525227 19.539023
##  [79]  8.151810  3.098777  4.944060  5.362978  2.057880 26.155842
##  [85] 14.053245  8.768882 10.215374 17.593198  3.270086  5.936323
##  [91] 13.524049  8.625081  3.824912  5.800611  5.591581  5.707430
##  [97]  3.987159  4.389560  9.471902  3.068005 30.970783 32.288310
## [103] 16.382188 27.303966 18.252965 14.211336 57.834820 27.316610
## [109] 17.104499 29.976800 42.865459 24.851409 25.740563 34.263970
## [115] 56.920919 43.839091 29.769508 30.314755  8.766718 35.514678
## [121] 27.338244 44.696221 18.955613 38.729456 26.599257 32.185485
## [127] 44.286331 45.304056 18.655092 42.634687 19.136274 40.640072
## [133] 20.651593 46.870404 53.923075 20.573812 41.472972 33.431351
## [139] 49.320574 31.907998 34.144669 57.844485 32.288310 20.731382
## [145] 39.082779 47.916444 32.925457 34.182320 45.599451 41.065734</code></pre>
<pre class="r"><code>mtcars %&gt;%
  insist_rows(maha_dist, within_n_mads(10), everything())</code></pre>
<pre><code>##                      mpg cyl  disp  hp drat    wt  qsec vs am gear
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4
##                     carb
## Mazda RX4              4
## Mazda RX4 Wag          4
## Datsun 710             1
## Hornet 4 Drive         1
## Hornet Sportabout      2
## Valiant                1
## Duster 360             4
## Merc 240D              2
## Merc 230               2
## Merc 280               4
## Merc 280C              4
## Merc 450SE             3
## Merc 450SL             3
## Merc 450SLC            3
## Cadillac Fleetwood     4
## Lincoln Continental    4
## Chrysler Imperial      4
## Fiat 128               1
## Honda Civic            2
## Toyota Corolla         1
## Toyota Corona          1
## Dodge Challenger       2
## AMC Javelin            2
## Camaro Z28             4
## Pontiac Firebird       2
## Fiat X1-9              1
## Porsche 914-2          2
## Lotus Europa           2
## Ford Pantera L         4
## Ferrari Dino           6
## Maserati Bora          8
## Volvo 142E             2</code></pre>
<pre class="r"><code>  ## anything here will run</code></pre>
</div>
</div>
<div id="robust-mahalanobis-distance" class="section level2">
<h2><span class="header-section-number">3.2</span> Robust Mahalanobis Distance</h2>
<p>A robust version of Mahalanobis distance using an Orthogonalized Gnanadesikan-Kettenring pairwise estimator (Gnanadesikan &amp; Kettenring, 1972).
<code>check_outliers</code> function in <code>{performance}</code> package has ‘Robust Mahalanobis Distance’ option. The <code>bigutilsr</code> package is required to apply this option.</p>
<p>Meanwhile, the <code>maha_sparse</code> function in <code>{DiPs}</code> package creates a robust Mahalanobis distance for matching based on a sparse network.</p>
<pre class="r"><code># load library
library(DiPs)

# pull data source
data(&quot;nh0506Homocysteine&quot;)
attach(nh0506Homocysteine)

X &lt;- cbind(female,age,black,education,povertyr,bmi)
p &lt;- glm(z~female+age+black+education+povertyr+bmi,family=binomial)$fitted.values
d &lt;- cbind(nh0506Homocysteine,p)
detach(nh0506Homocysteine)

# Apply symmetric caliper 0.15 on propensity score
dist1&lt;-maha_sparse(d$z,X,p,0.15)
length(dist1$d)</code></pre>
<pre><code>## [1] 863183</code></pre>
<pre class="r"><code># apply asymmetric caliper c(-0.2,0.1) on propensity score
dist2 &lt;- maha_sparse(d$z,X,p,c(-0.2,0.1))
length(dist2$d)</code></pre>
<pre><code>## [1] 751835</code></pre>
<p>You can also plot the Robust Mahalanobis distance through <code>plot.mcd</code> function from <code>{rrcov}</code> package. <code>plot.mcd</code> function shows the <strong>Mahalanobis distance</strong> based on <strong>robust</strong> and <strong>classical</strong> estimates of the location and the covariance matrix in different plots.</p>
<pre class="r"><code># load library.
library(rrcov)
data(hbk)
hbk.x &lt;- data.matrix(hbk[, 1:3])
datamcd &lt;- CovMcd(hbk.x)

rrcov::plot(datamcd)</code></pre>
<p><img src="figure/plot.mcd-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="minimum-covariance-determinant-mcd" class="section level2">
<h2><span class="header-section-number">3.3</span> Minimum Covariance Determinant (MCD)</h2>
<p>Another robust version of Mahalanobis. Leys et al. (2018) argue that Mahalanobis Distance is not a robust way to determine outliers, as it uses the means and covariances of all the data – including the outliers – to determine individual difference scores.</p>
<p>Minimum Covariance Determinant calculates the mean and covariance matrix based on the most central subset of the data (by default, <code>66%</code>), before computing the Mahalanobis Distance. This is deemed to be a more robust method of identifying and removing outliers than regular Mahalanobis distance.</p>
<ol style="list-style-type: decimal">
<li>MCD looks for those <code>h</code> observations whose classical covariance matrix has the lowest possible determinant.</li>
<li>MCD estimate of location is then mean of these <code>h</code> observations</li>
<li>MCD estimate of scatter is a sample covariance matrix of these <code>h</code> points (multiplied by consistency factor).</li>
<li>The re-weighting step is applied to improve efficiency at normal data.</li>
<li>The computation of MCD is difficult, but several fast algorithms are proposed.</li>
</ol>
<pre class="r"><code>Y_mcd &lt;- robustbase::covMcd(Y)
# Robust estimate of location
Y_mcd$center</code></pre>
<pre><code>##     body    brain 
## 3.028827 4.275608</code></pre>
<pre class="r"><code># Robust estimate of scatter
Y_mcd$cov</code></pre>
<pre><code>##           body    brain
## body  18.85849 14.16031
## brain 14.16031 11.03351</code></pre>
<p>By plugging in these robust estimates of location and scatter in the definition of the Mahalanobis distances, we obtain <strong>robust distances</strong> and can create a <strong>robust tolerance ellipsoid (RTE)</strong>.</p>
<div id="robust-tolerance-ellipsoid-rte" class="section level3">
<h3><span class="header-section-number">3.3.1</span> robust tolerance ellipsoid (RTE)</h3>
<pre class="r"><code>Y_mcd &lt;- robustbase::covMcd(Y)
ellipse_mcd &lt;- data.frame(car::ellipse(center = Y_mcd$center,
                                  shape = Y_mcd$cov,
                                  radius= Y_radius, 
                                  segments=100,draw=FALSE))
colnames(ellipse_mcd) &lt;- colnames(Y)
plot_fig &lt;- plot_fig +
  geom_polygon(data=ellipse_mcd, color=&quot;red&quot;, fill=&quot;red&quot;, 
  alpha=0.3) +
  geom_point(aes(x = Y_mcd$center[1], y = Y_mcd$center[2]),
             color = &quot;red&quot;, size = 6)
plot_fig</code></pre>
<p><img src="figure/rte-1.png" width="576" style="display: block; margin: auto;" />
### Distance-Distance plot
The distance-distance plot shows the robust distance of each observation versus its classical Mahalanobis distance, obtained immediately from MCD object.</p>
<pre class="r"><code>robustbase::plot(Y_mcd, which = &quot;dd&quot;)</code></pre>
<p><img src="figure/dd_plot-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="invariant-coordinate-selection-ics" class="section level2">
<h2><span class="header-section-number">3.4</span> Invariant Coordinate Selection (ICS)</h2>
<p>The outlier are detected using ICS, which by default uses an alpha threshold of <code>0.025</code> (corresponding to the <code>2.5%</code> most extreme observations) as a cut-off value for outliers classification. The ICS coefficient can be calculated through <code>ics.outlier()</code> function from <code>{ICSOutlier}</code>. Please refer to the help-file of <a href="https://rdrr.io/cran/ICSOutlier/man/ics.outlier.html">ICSOutlier::ics.outlier()</a> to get more details about this procedure. Note that <code>method = "ics"</code> requires both <code>ICS</code> and <code>ICSOutlier</code> to be installed, and that it takes some time to compute the results.</p>
<pre class="r"><code>library(ICSOutlier)

# Example of no outlier
set.seed(123)
X = matrix(rweibull(1000, 4, 4), 500, 2)
X = apply(X,2, function(x){ifelse(x&lt;5 &amp; x&gt;2, x, runif(sum(!(x&lt;5 &amp; x&gt;2)), 5, 5.5))}) 
icsX &lt;- ics2(X)
icsOutlierAG &lt;- ICSOutlier::ics.outlier(icsX, test = &quot;anscombe&quot;, 
                                        level.dist = 0.01, 
                                                level.test = 0.05, 
                                                mDist = 100, ncores = 1)
summary(icsOutlierAG)</code></pre>
<pre><code>## 
## ICS based on two scatter matrices and two location estimates
## S1:  MeanCov
## S2:  Mean3Cov4
## 
## Searching for a small proportion of outliers
## 
## Components selected at nominal level 0.05: 2
## Selection method: norm.test (anscombe.test)
## Number of outliers at nominal level 0.01: 0</code></pre>
<pre class="r"><code>plot(icsOutlierAG)</code></pre>
<p><img src="figure/ics-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>rm(.Random.seed)</code></pre>
</div>
<div id="optics" class="section level2">
<h2><span class="header-section-number">3.5</span> OPTICS</h2>
<p>The Ordering Points To Identify the <strong>Clustering Structure (OPTICS)</strong> algorithm (Ankerst et al., 1999) is using similar concepts to <strong>DBSCAN</strong> (an unsupervised clustering technique that can be used for outliers detection). The threshold argument is passed as minPts, which corresponds to the minimum size of a cluster. By default, this size is set at 2 times the number of columns (Sander et al., 1998). Compared to the others techniques, that will always detect several outliers (as these are usually defined as a percentage of extreme values), the <code>check_outliers</code> function in <code>{performance}</code> package in a different manner and won’t always detect outliers. Note that <code>method = "optics"</code> requires the <strong>dbscan</strong> package to be installed, and that it takes some time to compute the results.</p>
</div>
<div id="isolation-forest" class="section level2">
<h2><span class="header-section-number">3.6</span> Isolation Forest</h2>
<p>The outliers are detected using the anomaly score of an isolation forest (a class of random forest). The default threshold of <code>0.025</code> will classify as outliers the observations located at <code>qnorm(1-0.025) * MAD</code>) (a robust equivalent of SD) of the median (roughly corresponding to the <code>2.5%</code> most extreme observations). Requires the solitude package.</p>
</div>
<div id="local-outlier-factor" class="section level2">
<h2><span class="header-section-number">3.7</span> Local Outlier Factor</h2>
<p>Based on a <code>K</code> nearest neighbours algorithm, LOF compares the local density of an point to the local densities of its neighbors instead of computing a distance from the center (Breunig et al., 2000). Points that have a substantially lower density than their neighbors are considered outliers. A LOF score of approximately 1 indicates that density around the point is comparable to its neighbors. Scores significantly larger than 1 indicate outliers. The default threshold of <code>0.025</code> will classify as outliers the observations located at <span class="math inline">\(qnorm(1-0.025) \times SD\)</span>) of the log-transformed LOF distance. Requires the <code>dbscan</code> package.</p>
</div>
</div>
<div id="check_outliers-function-in-performance-r-package" class="section level1">
<h1><span class="header-section-number">4</span> ‘check_outliers’ function in {performance} R package</h1>
<p>The <code>check_outliers</code> function in the <code>{performance}</code> package (Lüdecke et al., 2019) contains multiple composite outlier score detection methods and is worthy of mention.</p>
<p>You can find the function details from the below link.</p>
<p><a href="https://easystats.github.io/performance/reference/check_outliers.html">check_outliers</a></p>
<p>In this function, all the default thresholds are set as below.</p>
<pre class="r"><code>library(performance)

# Univariate
check_outliers(mtcars$mpg)</code></pre>
<pre><code>## Warning: 4 outliers detected (cases 18, 19, 20, 28).</code></pre>
<pre class="r"><code>#&gt; Warning: 4 outliers detected (cases 18, 19, 20, 28).
#&gt; 

# Multivariate
# select only mpg and disp (continuous)
mt1 &lt;- mtcars[, c(1, 3, 4)]
# create some fake outliers and attach outliers to main df
mt2 &lt;- rbind(mt1, data.frame(mpg = c(37, 40), disp = c(300, 400), hp = c(110, 120)))
# fit model with outliers
model &lt;- lm(disp ~ mpg + hp, data = mt2)

ol &lt;- check_outliers(model)
# plot(ol)
insight::get_data(mode)[ol, ]</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>#&gt; Warning: Could not get model data.
#&gt; NULL


check_outliers(model, method = c(&quot;mahalabonis&quot;, &quot;mcd&quot;))</code></pre>
<pre><code>## Warning: 6 outliers detected (cases 18, 20, 28, 31, 33, 34).</code></pre>
<pre class="r"><code>#&gt; Warning: 6 outliers detected (cases 18, 20, 28, 31, 33, 34).
#&gt; 
if (FALSE) {
# This one takes some seconds to finish...
check_outliers(model, method = &quot;ics&quot;)

# For dataframes
check_outliers(mtcars)
check_outliers(mtcars, method = &quot;all&quot;)
}</code></pre>
<div id="threshold-specification" class="section level3">
<h3><span class="header-section-number">4.0.1</span> Threshold specification</h3>
<pre class="r"><code># list of default Threshold specification. 
list(zscore = stats::qnorm(p = 1 - 0.025), 
     iqr = 1.5, 
     cook = stats::qf(0.5, ncol(x), nrow(x) - ncol(x)), 
     pareto = 0.7, 
     mahalanobis = stats::qchisq(p = 1 - 0.025, df = ncol(x)), 
     robust = stats::qchisq(p = 1 - 0.025, df = ncol(x)), 
     mcd = stats::qchisq(p = 1 - 0.025, df = ncol(x)), 
     ics = 0.025, 
     optics = 2 * ncol(x), 
     iforest = 0.025, 
     lof = 0.025 )</code></pre>
<pre><code>## Error in ncol(x): object &#39;x&#39; not found</code></pre>
<p><em>Note: Some of the contents are originally from Mishtert T and Will Hipson online tutorials. If you are interested in, you can find both online tutorials below.</em></p>
<p><em><a href="https://medium.com/towards-artificial-intelligence/outlier-detection-part-2-multivariate-df486f658d09">Mishtert T</a></em></p>
<p><em><a href="https://www.r-bloggers.com/2019/01/a-new-way-to-handle-multivariate-outliers/">Will Hipson</a></em></p>
<p>– To be Continued</p>
</div>
</div>
<div id="reference" class="section level1">
<h1><span class="header-section-number">5</span> Reference</h1>
<ul>
<li><p>Archimbaud, A., Nordhausen, K., &amp; Ruiz-Gazen, A. (2018). ICS for multivariate outlier detection with application to quality control. Computational Statistics &amp; Data Analysis, 128, 184–199. doi: 10.1016/j.csda.2018.06.011</p></li>
<li><p>Gnanadesikan, R., &amp; Kettenring, J. R. (1972). Robust estimates, residuals, and outlier detection with multiresponse data. Biometrics, 81-124.</p></li>
<li><p>Bollen, K. A., &amp; Jackman, R. W. (1985). Regression diagnostics: An expository treatment of outliers and influential cases. Sociological Methods &amp; Research, 13(4), 510-542.</p></li>
<li><p>Cabana, E., Lillo, R. E., &amp; Laniado, H. (2019). Multivariate outlier detection based on a robust Mahalanobis distance with shrinkage estimators. arXiv preprint arXiv:1904.02596.</p></li>
<li><p>Cook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15-18.</p></li>
<li><p>Iglewicz, B., &amp; Hoaglin, D. C. (1993). How to detect and handle outliers (Vol. 16). Asq Press.</p></li>
<li><p>Leys, C., Klein, O., Dominicy, Y., &amp; Ley, C. (2018). Detecting multivariate outliers: Use a robust variant of Mahalanobis distance. Journal of Experimental Social Psychology, 74, 150-156.</p></li>
<li><p>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008, December). Isolation forest. In 2008 Eighth IEEE International Conference on Data Mining (pp. 413-422). IEEE.</p></li>
<li><p>Rousseeuw, P. J., &amp; Van Zomeren, B. C. (1990). Unmasking multivariate outliers and leverage points. Journal of the American Statistical association, 85(411), 633-639.</p></li>
</ul>
</div>
